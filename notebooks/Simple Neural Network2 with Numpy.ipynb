{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "\n",
    "[Example from MLNotebook - A simple Nerual Network with numpy in python](https://mlnotebook.github.io/post/nn-in-python/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\t [0.49690881] \t[0.]\n",
      "[1 1]\t [0.51584919] \t[0.]\n",
      "[0 1]\t [0.47687406] \t[1.]\n",
      "[1 0]\t [0.53584412] \t[1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "    \n",
    "class BackPropagationNetwork:\n",
    "    num_layers = 0\n",
    "    shape = None\n",
    "    weights = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x, Derivative=False):\n",
    "        \"\"\"\n",
    "        sigmoid maps the input to a value between 0 and 1 but not equal to 0 or 1.\n",
    "        It means output will be high signal if positive and low signal if negative.\n",
    "        Simoid's natural threshold is o.5, so any input above will be hig or 1 in binary.\n",
    "        \"\"\"\n",
    "        if not Derivative:\n",
    "            return 1 / (1 + np.exp (-x))\n",
    "        else:\n",
    "            out = BackPropagationNetwork.sigmoid(x)\n",
    "            return out * (1 - out)\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_nodes):\n",
    "        self.num_layers = len(num_nodes) - 1\n",
    "        self.shape = num_nodes\n",
    "        self._layer_input = []\n",
    "        self._layer_output = []\n",
    "        self._prev_weight_delta = []\n",
    "        \n",
    "        for (l1, l2) in zip(num_nodes[:-1], num_nodes[1:]):\n",
    "            self.weights.append(np.random.normal(scale=0.1, size=(l2, l1 + 1)))\n",
    "            self._prev_weight_delta.append(np.zeros((l2, l1+1)))\n",
    "        \n",
    "    \n",
    "    def fp(self, i_data):\n",
    "        \"\"\"\n",
    "        Forward pass get input and run it through the NN\n",
    "        \n",
    "        O⃗j = σ(Wij O⃗I)\n",
    "        \n",
    "        σ is the activation transfer function or sigmoid in this case,\n",
    "        which is applied element wise to the product of the matrices.\n",
    "        \n",
    "        I is our input layer\n",
    "        J is our hidden layer\n",
    "        Wij is the weight connecting the ith node in I to the jth node in J\n",
    "    \n",
    "        \"\"\"\n",
    "        delta = []\n",
    "        num_examples = i_data.shape[0]\n",
    "        # clean values from prevous layer\n",
    "        self._layer_input = []\n",
    "        self._layer_output = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                layer_input = self.weights[0].dot(np.vstack([i_data.T, np.ones([1, num_examples])]))\n",
    "            else:\n",
    "                layer_input = self.weights[i].dot(np.vstack([i_data.T, np.ones([1, num_examples])]))\n",
    "            \n",
    "            self._layer_input.append(layer_input)\n",
    "            self._layer_output.append(self.sigmoid(layer_input))\n",
    "                \n",
    "        return self._layer_output[-1].T\n",
    "        \n",
    "        \n",
    "    def bp(self, i_data, target, t_rate = 0.2, momentum=0.5):\n",
    "        \"\"\"Train epoch (back propagation) get the error, deltas\n",
    "        and back propagate to update the weights\"\"\"\n",
    "        delta = []\n",
    "        num_examples = i_data.shape[0]\n",
    "        self.fp(i_data)\n",
    "        \n",
    "        # Calculate the deltas\n",
    "        # δ⃗ K=σ′(WJKO⃗ J)∗(O⃗ K−TK)\n",
    "        # This is back propagation, we use the reversed function to ensure\n",
    "        # the algorithm considers the ayers in reverse order.\n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            if i == self.num_layers - 1:\n",
    "                # If the output layer, then compare to the target values\n",
    "                o_delta = self._layer_output[i] - target.T\n",
    "                error = np.sum(o_delta**2)\n",
    "                delta.append(o_delta * self.sigmoid(self._layer_input[i], True))\n",
    "            else:\n",
    "                delta_pullback = self.weights[i + 1].T.dot(delta[-1])\n",
    "                delta.append(delta_pullback[:-1,:] * self.sigmoid(self._layer_input[i], True))\n",
    "\n",
    "        # Compute updates to each weight\n",
    "        for i in range(self.num_layers):\n",
    "            delta_index = self.num_layers - 1 - i    \n",
    "            if i == 0:\n",
    "               layer_output = np.vstack([i_data.T, np.ones([1, num_examples])])\n",
    "            else:\n",
    "                # If a hidden layer. compare to the following layer's delta\n",
    "                layer_output = np.vstack([self._layer_output[i - 1], np.ones([1, self._layer_output[i - 1].shape[1]])])\n",
    "            \n",
    "            # Update wights\n",
    "            # δ⃗ J = σ′(WIJOI)∗W⊺JKδ⃗ K\n",
    "            this_weight_deta = np.sum(layer_output[None,:,:].transpose(2, 0, 1) * delta[delta_index][None,:,:].transpose(2, 1, 0), axis = 0)\n",
    "            weight_delta = t_rate * this_weight_deta + momentum * self._prev_weight_delta[i]\n",
    "            \n",
    "            self.weights[i] -= weight_delta\n",
    "            self._prev_weight_delta[i] = weight_delta\n",
    "            \n",
    "        return error\n",
    "\n",
    "inp = np.array([[0,0],[1,1],[0,1],[1,0]])\n",
    "tar = np.array([[0.0],[0.0],[1.0],[1.0]])\n",
    "\n",
    "bpn = BackPropagationNetwork((2,2,1))\n",
    "\n",
    "Error = bpn.bp(inp, tar)\n",
    "Output = bpn.fp(inp)\n",
    "for i in range(inp.shape[0]):\n",
    "    print('{0}\\t {1} \\t{2}'.format(inp[i], Output[i], tar[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration\n",
    "\n",
    "We just tell our algorithm to repeat a maximum of `max_iterations` times or until the `error` is below `min_error` (whichever comes first). As the weights are stored internally within NN every time we call the `bp` method, it uses the latest, internally stored weights and doesn’t start again - the weights are only initialised once upon creation of NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\tError: 1.002120\n",
      "Iteration 2500\tError: 1.012917\n",
      "Iteration 5000\tError: 1.006105\n",
      "Iteration 7500\tError: 1.004468\n",
      "Iteration 10000\tError: 1.003674\n",
      "[0 0]\t [0.02274466] \t[0.]\n",
      "[1 1]\t [0.03816949] \t[0.]\n",
      "[0 1]\t [4.06987801e-05] \t[1.]\n",
      "[1 0]\t [0.95779328] \t[1.]\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 10000\n",
    "min_error = 1e-5\n",
    "error = None\n",
    "bpn = BackPropagationNetwork((2,2,1))\n",
    "for i in range(max_iterations + 1):\n",
    "    error = bpn.bp(inp, tar)\n",
    "    if i % 2500 == 0:\n",
    "        print(\"Iteration {0}\\tError: {1:0.6f}\".format(i, error))\n",
    "    if error <= min_error:\n",
    "        print(\"Minimum error reached at iteration {0}\".format(i))\n",
    "        break\n",
    "\n",
    "Output = bpn.fp(inp)\n",
    "for i in range(inp.shape[0]):\n",
    "    print('{0}\\t {1} \\t{2}'.format(inp[i], Output[i], tar[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
