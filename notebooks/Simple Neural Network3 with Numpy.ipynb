{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "\n",
    "[Example from MLNotebook - A simple Nerual Network with numpy in python](https://mlnotebook.github.io/post/nn-in-python/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\t [0.24688912] \t[0.]\n",
      "[1 1]\t [0.12012452] \t[0.]\n",
      "[0 1]\t [0.06548366] \t[1.]\n",
      "[1 0]\t [0.30152999] \t[1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "    \n",
    "class BackPropagationNetwork:\n",
    "    num_layers = 0\n",
    "    shape = None\n",
    "    weights = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x, Derivative=False):\n",
    "        \"\"\"\n",
    "        sigmoid maps the input to a value between 0 and 1 but not equal to 0 or 1.\n",
    "        It means output will be high signal if positive and low signal if negative.\n",
    "        Simoid's natural threshold is o.5, so any input above will be hig or 1 in binary.\n",
    "        \"\"\"\n",
    "        if not Derivative:\n",
    "            return 1 / (1 + np.exp (-x))\n",
    "        else:\n",
    "            out = BackPropagationNetwork.sigmoid(x)\n",
    "            return out * (1 - out)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def linear(x, Derivative=False):\n",
    "        \"\"\"\n",
    "        Linear identity f(Xi) = Xi used when we want a node to give output\n",
    "        without applying any thresholds. Commonly used in final output layer nodes.\n",
    "\n",
    "        Not used in hidden layers, else output will be only a separable solution.\n",
    "        \"\"\"\n",
    "        if not Derivative:\n",
    "            return x\n",
    "        else:\n",
    "            return 1.0\n",
    "        \n",
    "    @staticmethod\n",
    "    def relu(x, Derivative=False):\n",
    "        \"\"\"\n",
    "        relu is a popular special case of ramp called Rectifying Linear Unit.\n",
    "        in relu T1 = 0 and T2 is the maximum of the input given a linear function\n",
    "        with no negative values.\n",
    "        \"\"\"\n",
    "        if not Derivative:\n",
    "            return np.maximum(0,x)\n",
    "        else:\n",
    "            out = np.ones(x.shape)\n",
    "            out[(x < 0)] = 0\n",
    "            return out\n",
    "\n",
    "    \n",
    "    def __init__(self, num_nodes, tfs=None):\n",
    "        self.num_layers = len(num_nodes) - 1\n",
    "        self.shape = num_nodes\n",
    "        \n",
    "        if tfs is None:\n",
    "            layer_tf = []\n",
    "            for i in range(self.num_layers):\n",
    "                if i == self.num_layers - 1:\n",
    "                    layer_tf.append(BackPropagationNetwork.linear)\n",
    "                else:\n",
    "                    layer_tf.append(BackPropagationNetwork.sigmoid)\n",
    "        else:\n",
    "            if len(num_nodes) != len(tfs):\n",
    "                raise ValueError(\"Number of TFs must match number of layers\")\n",
    "            elif tfs[0] is not None:\n",
    "                raise ValueError(\"The input layer does not need a TF\")\n",
    "            else:\n",
    "                layer_tf = tfs[1:]\n",
    "                \n",
    "        self.tfs = layer_tf\n",
    "        self._layer_input = []\n",
    "        self._layer_output = []\n",
    "        self._prev_weight_delta = []\n",
    "        \n",
    "        for (l1, l2) in zip(num_nodes[:-1], num_nodes[1:]):\n",
    "            self.weights.append(np.random.normal(scale=0.1, size=(l2, l1 + 1)))\n",
    "            self._prev_weight_delta.append(np.zeros((l2, l1+1)))\n",
    "        \n",
    "    \n",
    "    def fp(self, i_data):\n",
    "        \"\"\"\n",
    "        Forward pass get input and run it through the NN\n",
    "        \n",
    "        O⃗j = σ(Wij O⃗I)\n",
    "        \n",
    "        σ is the activation transfer function or sigmoid in this case,\n",
    "        which is applied element wise to the product of the matrices.\n",
    "        \n",
    "        I is our input layer\n",
    "        J is our hidden layer\n",
    "        Wij is the weight connecting the ith node in I to the jth node in J\n",
    "    \n",
    "        \"\"\"\n",
    "        delta = []\n",
    "        num_examples = i_data.shape[0]\n",
    "        # clean values from prevous layer\n",
    "        self._layer_input = []\n",
    "        self._layer_output = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                layer_input = self.weights[0].dot(np.vstack([i_data.T, np.ones([1, num_examples])]))\n",
    "            else:\n",
    "                layer_input = self.weights[i].dot(np.vstack([i_data.T, np.ones([1, num_examples])]))\n",
    "            \n",
    "            self._layer_input.append(layer_input)\n",
    "            self._layer_output.append(self.tfs[i](layer_input))\n",
    "                \n",
    "        return self._layer_output[-1].T\n",
    "        \n",
    "        \n",
    "    def bp(self, i_data, target, t_rate = 0.2, momentum=0.5):\n",
    "        \"\"\"Train epoch (back propagation) get the error, deltas\n",
    "        and back propagate to update the weights\"\"\"\n",
    "        delta = []\n",
    "        num_examples = i_data.shape[0]\n",
    "        self.fp(i_data)\n",
    "        \n",
    "        # Calculate the deltas\n",
    "        # δ⃗ K=σ′(WJKO⃗ J)∗(O⃗ K−TK)\n",
    "        # This is back propagation, we use the reversed function to ensure\n",
    "        # the algorithm considers the ayers in reverse order.\n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            if i == self.num_layers - 1:\n",
    "                # If the output layer, then compare to the target values\n",
    "                o_delta = self._layer_output[i] - target.T\n",
    "                error = np.sum(o_delta**2)\n",
    "                delta.append(o_delta * self.tfs[i](self._layer_input[i], True))\n",
    "            else:\n",
    "                delta_pullback = self.weights[i + 1].T.dot(delta[-1])\n",
    "                delta.append(delta_pullback[:-1,:] * self.tfs[i](self._layer_input[i], True))\n",
    "\n",
    "        # Compute updates to each weight\n",
    "        for i in range(self.num_layers):\n",
    "            delta_index = self.num_layers - 1 - i    \n",
    "            if i == 0:\n",
    "               layer_output = np.vstack([i_data.T, np.ones([1, num_examples])])\n",
    "            else:\n",
    "                # If a hidden layer. compare to the following layer's delta\n",
    "                layer_output = np.vstack([self._layer_output[i - 1], np.ones([1, self._layer_output[i - 1].shape[1]])])\n",
    "            \n",
    "            # Update wights\n",
    "            # δ⃗ J = σ′(WIJOI)∗W⊺JKδ⃗ K\n",
    "            this_weight_deta = np.sum(layer_output[None,:,:].transpose(2, 0, 1) * delta[delta_index][None,:,:].transpose(2, 1, 0), axis = 0)\n",
    "            weight_delta = t_rate * this_weight_deta + momentum * self._prev_weight_delta[i]\n",
    "            \n",
    "            self.weights[i] -= weight_delta\n",
    "            self._prev_weight_delta[i] = weight_delta\n",
    "            \n",
    "        return error\n",
    "\n",
    "inp = np.array([[0,0],[1,1],[0,1],[1,0]])\n",
    "tar = np.array([[0.0],[0.0],[1.0],[1.0]])\n",
    "tfs = [None,\n",
    "               BackPropagationNetwork.relu,\n",
    "               BackPropagationNetwork.relu]\n",
    "\n",
    "bpn = BackPropagationNetwork((2,2,1), tfs)\n",
    "\n",
    "Error = bpn.bp(inp, tar)\n",
    "Output = bpn.fp(inp)\n",
    "for i in range(inp.shape[0]):\n",
    "    print('{0}\\t {1} \\t{2}'.format(inp[i], Output[i], tar[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration\n",
    "\n",
    "We just tell our algorithm to repeat a maximum of `max_iterations` times or until the `error` is below `min_error` (whichever comes first). As the weights are stored internally within NN every time we call the `bp` method, it uses the latest, internally stored weights and doesn’t start again - the weights are only initialised once upon creation of NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\tError: 1.436565\n",
      "Iteration 2500\tError: 1.343291\n",
      "Iteration 5000\tError: 1.343291\n",
      "Iteration 7500\tError: 1.343291\n",
      "Iteration 10000\tError: 1.343291\n",
      "[0 0]\t [0.] \t[0.]\n",
      "[1 1]\t [0.41430142] \t[0.]\n",
      "[0 1]\t [0.] \t[1.]\n",
      "[1 0]\t [0.58569858] \t[1.]\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 10000\n",
    "min_error = 1e-5\n",
    "error = None\n",
    "bpn = BackPropagationNetwork((2,2,1), tfs)\n",
    "for i in range(max_iterations + 1):\n",
    "    error = bpn.bp(inp, tar)\n",
    "    if i % 2500 == 0:\n",
    "        print(\"Iteration {0}\\tError: {1:0.6f}\".format(i, error))\n",
    "    if error <= min_error:\n",
    "        print(\"Minimum error reached at iteration {0}\".format(i))\n",
    "        break\n",
    "\n",
    "Output = bpn.fp(inp)\n",
    "for i in range(inp.shape[0]):\n",
    "    print('{0}\\t {1} \\t{2}'.format(inp[i], Output[i], tar[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
